# ETL Orchestration Jobs
# Orchestrates the complete insurance analytics pipeline

resources:
  jobs:
    insurance_etl_full_refresh:
      name: insurance_${var.env}_etl_full_refresh
      
      description: "Complete ETL pipeline for insurance analytics - Bronze to Gold"
      
      tags:
        environment: ${var.env}
        project: insurance-analytics
        pipeline_type: batch
      
      max_concurrent_runs: 1
      
      timeout_seconds: 14400  # 4 hours
      
      schedule:
        quartz_cron_expression: "0 0 2 * * ?"  # Daily at 2 AM
        timezone_id: "America/New_York"
        pause_status: PAUSED
      
      email_notifications:
        on_start:
          - data-engineering@insurance-company.com
        on_success:
          - data-engineering@insurance-company.com
        on_failure:
          - data-engineering@insurance-company.com
          - data-engineering-oncall@insurance-company.com
      
      tasks:
        # Task 1: Generate Bronze Layer Data
        - task_key: generate_bronze_customers
          description: "Generate realistic customer data in bronze layer"
          
          notebook_task:
            notebook_path: ../src/bronze/generate_customers_data.py
            base_parameters:
              catalog: ${var.catalog_prefix}_bronze
          
          new_cluster:
            spark_version: "13.3.x-scala2.12"
            node_type_id: ${var.cluster_node_type}
            num_workers: ${var.min_workers}
            spark_conf:
              spark.databricks.delta.optimizeWrite.enabled: "true"
              spark.databricks.delta.autoCompact.enabled: "true"
          
          timeout_seconds: 1800
        
        - task_key: generate_bronze_policies
          description: "Generate realistic policy data in bronze layer"
          depends_on:
            - task_key: generate_bronze_customers
          
          notebook_task:
            notebook_path: ../src/bronze/generate_policies_data.py
            base_parameters:
              catalog: ${var.catalog_prefix}_bronze
          
          new_cluster:
            spark_version: "13.3.x-scala2.12"
            node_type_id: ${var.cluster_node_type}
            num_workers: ${var.min_workers}
          
          timeout_seconds: 2400
        
        - task_key: generate_bronze_claims
          description: "Generate realistic claims data in bronze layer"
          depends_on:
            - task_key: generate_bronze_policies
          
          notebook_task:
            notebook_path: ../src/bronze/generate_claims_data.py
            base_parameters:
              catalog: ${var.catalog_prefix}_bronze
          
          new_cluster:
            spark_version: "13.3.x-scala2.12"
            node_type_id: ${var.cluster_node_type}
            num_workers: ${var.min_workers}
          
          timeout_seconds: 2400
        
        # Task 2: Create Silver Layer Tables
        - task_key: create_silver_tables
          description: "Create silver layer table structures"
          depends_on:
            - task_key: generate_bronze_claims
          
          sql_task:
            file:
              path: ../src/setup/02_create_silver_tables.sql
            parameters:
              catalog: ${var.catalog_prefix}_silver
          
          sql_task_warehouse_id: "${var.sql_warehouse_id}"
          
          timeout_seconds: 1200
        
        # Task 3: Run DLT Pipeline
        - task_key: run_bronze_to_silver_pipeline
          description: "Execute Delta Live Tables pipeline for bronze to silver transformation"
          depends_on:
            - task_key: create_silver_tables
          
          pipeline_task:
            pipeline_id: "${resources.pipelines.insurance_dlt_bronze_to_silver.id}"
            full_refresh: false
          
          timeout_seconds: 3600
        
        # Task 4: Apply Security (RLS/CLS)
        - task_key: apply_security
          description: "Create RLS and CLS functions and secure views"
          depends_on:
            - task_key: run_bronze_to_silver_pipeline
          
          sql_task:
            file:
              path: ../src/setup/03_create_security_rls_cls.sql
            parameters:
              catalog: ${var.catalog_prefix}_silver
          
          sql_task_warehouse_id: "${var.sql_warehouse_id}"
          
          timeout_seconds: 900
        
        # Task 5: Create Gold Layer
        - task_key: create_gold_tables
          description: "Create gold layer analytics tables"
          depends_on:
            - task_key: apply_security
          
          sql_task:
            file:
              path: ../src/setup/04_create_gold_tables.sql
            parameters:
              catalog: ${var.catalog_prefix}_gold
          
          sql_task_warehouse_id: "${var.sql_warehouse_id}"
          
          timeout_seconds: 1200
        
        # Task 6: Populate Gold Analytics
        - task_key: populate_customer_360
          description: "Build customer 360 analytics"
          depends_on:
            - task_key: create_gold_tables
          
          notebook_task:
            notebook_path: ../src/gold/build_customer_360.py
            base_parameters:
              silver_catalog: ${var.catalog_prefix}_silver
              gold_catalog: ${var.catalog_prefix}_gold
          
          new_cluster:
            spark_version: "13.3.x-scala2.12"
            node_type_id: ${var.cluster_node_type}
            num_workers: ${var.max_workers}
            spark_conf:
              spark.databricks.adaptive.enabled: "true"
          
          timeout_seconds: 2400
        
        - task_key: populate_fraud_detection
          description: "Run fraud detection analytics"
          depends_on:
            - task_key: create_gold_tables
          
          notebook_task:
            notebook_path: ../src/gold/build_fraud_detection.py
            base_parameters:
              silver_catalog: ${var.catalog_prefix}_silver
              gold_catalog: ${var.catalog_prefix}_gold
          
          new_cluster:
            spark_version: "13.3.x-scala2.12"
            node_type_id: ${var.cluster_node_type}
            num_workers: ${var.max_workers}
          
          timeout_seconds: 2400
        
        # Task 7: Data Quality Validation
        - task_key: validate_data_quality
          description: "Run data quality checks across all layers"
          depends_on:
            - task_key: populate_customer_360
            - task_key: populate_fraud_detection
          
          notebook_task:
            notebook_path: ../src/analytics/data_quality_validation.py
            base_parameters:
              environment: ${var.env}
          
          existing_cluster_id: "${var.shared_cluster_id}"
          
          timeout_seconds: 600
        
        # Task 8: Send Success Notification
        - task_key: send_completion_report
          description: "Send pipeline completion report"
          depends_on:
            - task_key: validate_data_quality
          
          notebook_task:
            notebook_path: ../src/analytics/pipeline_completion_report.py
            base_parameters:
              job_run_id: "{{job.run_id}}"
              environment: ${var.env}
          
          existing_cluster_id: "${var.shared_cluster_id}"
          
          timeout_seconds: 300
      
      permissions:
        - level: CAN_VIEW
          group_name: data_engineers
        - level: CAN_MANAGE_RUN
          group_name: data_engineering_leads
        - level: CAN_MANAGE
          group_name: insurance_platform_admins

    # Incremental Refresh Job
    insurance_etl_incremental:
      name: insurance_${var.env}_etl_incremental
      
      description: "Incremental ETL pipeline for insurance analytics"
      
      tags:
        environment: ${var.env}
        project: insurance-analytics
        pipeline_type: incremental
      
      max_concurrent_runs: 1
      
      schedule:
        quartz_cron_expression: "0 */2 * * * ?"  # Every 2 hours
        timezone_id: "America/New_York"
        pause_status: PAUSED
      
      tasks:
        - task_key: run_incremental_dlt
          description: "Run DLT pipeline in incremental mode"
          
          pipeline_task:
            pipeline_id: "${resources.pipelines.insurance_dlt_bronze_to_silver.id}"
            full_refresh: false
          
          timeout_seconds: 1800
        
        - task_key: refresh_gold_analytics
          description: "Incrementally refresh gold layer"
          depends_on:
            - task_key: run_incremental_dlt
          
          notebook_task:
            notebook_path: ../src/gold/incremental_refresh.py
            base_parameters:
              silver_catalog: ${var.catalog_prefix}_silver
              gold_catalog: ${var.catalog_prefix}_gold
              refresh_type: incremental
          
          new_cluster:
            spark_version: "13.3.x-scala2.12"
            node_type_id: ${var.cluster_node_type}
            num_workers: 2
          
          timeout_seconds: 1200

